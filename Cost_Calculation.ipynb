{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPpQATajc+1aByI6DQBN6q9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khunapat/Exxonmobile-Hackhaton/blob/main/Cost_Calculation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seperate Inventory by month"
      ],
      "metadata": {
        "id": "KUPjKb3jZaMf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQKnsjiBZLn6",
        "outputId": "7ee7d5a4-0092-4bbf-c34e-33ccf9770767"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been saved to: /content/Inventory\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Load the Inventory data (replace the path with your file path in Colab)\n",
        "file_path = '/content/Inventory.csv'  # Update this with the correct path\n",
        "inventory_data = pd.read_csv(file_path, parse_dates=['BALANCE_AS_OF_DATE'])\n",
        "\n",
        "# Create a new directory to store the separated files\n",
        "output_directory = '/content/Inventory'  # Update this with your desired path\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "# Extract year-month from the 'BALANCE_AS_OF_DATE' column\n",
        "inventory_data['Month'] = inventory_data['BALANCE_AS_OF_DATE'].dt.to_period('M')\n",
        "\n",
        "# Loop through each unique month and PLANT_NAME to save the data to new CSV files\n",
        "for month, month_data in inventory_data.groupby('Month'):\n",
        "    month_folder = os.path.join(output_directory, str(month))\n",
        "    os.makedirs(month_folder, exist_ok=True)\n",
        "\n",
        "    for plant, plant_data in month_data.groupby('PLANT_NAME'):\n",
        "        plant_file_path = os.path.join(month_folder, f'{plant}_inventory.csv')\n",
        "        plant_data.to_csv(plant_file_path, index=False)\n",
        "\n",
        "# Verify the structure of the output directory\n",
        "print(f\"Data has been saved to: {output_directory}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sum Inventory my month"
      ],
      "metadata": {
        "id": "6a8zjVUbabTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 1)  CONFIGURE PATHS\n",
        "# ------------------------------------------------------------------\n",
        "inventory_csv = '/content/Inventory.csv'          # raw Inventory file\n",
        "output_dir    = '/content/Inventory'      # where summary will live\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2)  LOAD & PREP\n",
        "# ------------------------------------------------------------------\n",
        "df = pd.read_csv(inventory_csv, parse_dates=['BALANCE_AS_OF_DATE'])\n",
        "\n",
        "# Convert KG → metric tonnes (MT) if needed\n",
        "df['UNRESRICTED_STOCK_MT'] = df['UNRESRICTED_STOCK'] / 1000.0\n",
        "\n",
        "# Month key: YYYY-MM\n",
        "df['Month'] = df['BALANCE_AS_OF_DATE'].dt.to_period('M').astype(str)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 3)  SUM BY MONTH + WAREHOUSE\n",
        "# ------------------------------------------------------------------\n",
        "summary = (df.groupby(['Month', 'PLANT_NAME'])['UNRESRICTED_STOCK_MT']\n",
        "             .sum()\n",
        "             .reset_index()\n",
        "             .rename(columns={\n",
        "                 'PLANT_NAME': 'Warehouse',\n",
        "                 'UNRESRICTED_STOCK_MT': 'Total_UNRESRICTED_STOCK_MT'\n",
        "             })\n",
        "             .sort_values(['Month', 'Warehouse'])\n",
        "             .reset_index(drop=True))\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 4)  SAVE ONE MERGED FILE\n",
        "# ------------------------------------------------------------------\n",
        "output_file = os.path.join(output_dir, 'merged_inventory_summary.csv')\n",
        "summary.to_csv(output_file, index=False)\n",
        "\n",
        "print(f'✅  Summary created with {len(summary)} rows and saved to:\\n{output_file}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgnbIcKdafZV",
        "outputId": "e43e91e3-724d-47d0-cbac-00c81d0bf2f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅  Summary created with 26 rows and saved to:\n",
            "/content/Inventory/merged_inventory_summary.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inbound Date Classifying"
      ],
      "metadata": {
        "id": "6vy9QMsofJjA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 1)  CONFIGURE PATHS\n",
        "# ------------------------------------------------------------------\n",
        "inbound_csv = '/content/Inbound.csv'         # raw file\n",
        "output_dir  = '/content/Inbound'             # folder to drop the 2 files\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2)  LOAD DATA\n",
        "# ------------------------------------------------------------------\n",
        "df = pd.read_csv(inbound_csv, parse_dates=['INBOUND_DATE'])\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 3)  FIXED DATE RANGE  (01-Nov-2023  →  31-Jan-2025)\n",
        "# ------------------------------------------------------------------\n",
        "date_min = pd.Timestamp('2023-11-01')\n",
        "date_max = pd.Timestamp('2025-01-31')\n",
        "full_index = pd.date_range(date_min, date_max, freq='D')\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 4)  BUILD ONE DAILY-TOTAL FILE PER PLANT\n",
        "# ------------------------------------------------------------------\n",
        "for plant, g in df.groupby('PLANT_NAME'):\n",
        "    daily_tot = (\n",
        "        g.groupby(g['INBOUND_DATE'].dt.normalize())['NET_QUANTITY_MT']\n",
        "          .sum()\n",
        "          .rename('Total_NET_QUANTITY_MT')\n",
        "          .reindex(full_index, fill_value=0)        # fill missing days\n",
        "    )\n",
        "\n",
        "    out_df = daily_tot.reset_index().rename(columns={'index': 'Date'})\n",
        "    out_df.insert(1, 'Warehouse', plant)\n",
        "\n",
        "    out_file = os.path.join(output_dir,\n",
        "                            f'inbound_daily_totals_{plant}.csv')\n",
        "    out_df.to_csv(out_file, index=False)\n",
        "    print(f'✅  Saved {out_file}')\n",
        "\n",
        "print('\\n✔️  Files now cover 01-Nov-2023 → 31-Jan-2025 for each warehouse.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62SZY8oLbkuE",
        "outputId": "205c6d46-6d2e-4bf3-c56d-1ecdf47605ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅  Saved /content/Inbound/inbound_daily_totals_CHINA-WAREHOUSE.csv\n",
            "✅  Saved /content/Inbound/inbound_daily_totals_SINGAPORE-WAREHOUSE.csv\n",
            "\n",
            "✔️  Files now cover 01-Nov-2023 → 31-Jan-2025 for each warehouse.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-1791026541.py:14: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
            "  df = pd.read_csv(inbound_csv, parse_dates=['INBOUND_DATE'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 1)  CONFIGURE PATHS\n",
        "# ------------------------------------------------------------------\n",
        "outbound_csv = '/content/Outbound.csv'     # raw outbound data\n",
        "root_dir     = '/content/Outbound'         # master output folder\n",
        "\n",
        "by_mode_dir      = os.path.join(root_dir, 'by_mode')\n",
        "warehouse_all_dir = os.path.join(root_dir, 'warehouse_all')\n",
        "\n",
        "os.makedirs(by_mode_dir, exist_ok=True)\n",
        "os.makedirs(warehouse_all_dir, exist_ok=True)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2)  LOAD DATA\n",
        "# ------------------------------------------------------------------\n",
        "df = pd.read_csv(outbound_csv, parse_dates=['OUTBOUND_DATE'])\n",
        "df.rename(columns=str.upper, inplace=True)   # uniform caps\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 3)  FIXED DATE WINDOW  (01-Nov-2023 → 31-Jan-2025)\n",
        "# ------------------------------------------------------------------\n",
        "date_min = pd.Timestamp('2023-11-01')\n",
        "date_max = pd.Timestamp('2025-01-31')\n",
        "full_index = pd.date_range(date_min, date_max, freq='D')\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 4)  A)  BY-MODE  (warehouse × transport mode)  →  4 files\n",
        "# ------------------------------------------------------------------\n",
        "for (plant, mode), g in df.groupby(['PLANT_NAME', 'MODE_OF_TRANSPORT']):\n",
        "    daily_tot = (\n",
        "        g.groupby(g['OUTBOUND_DATE'].dt.normalize())['NET_QUANTITY_MT']\n",
        "          .sum()\n",
        "          .rename('Total_NET_QUANTITY_MT')\n",
        "          .reindex(full_index, fill_value=0)\n",
        "    )\n",
        "\n",
        "    out_df = daily_tot.reset_index().rename(columns={'index': 'Date'})\n",
        "    out_df.insert(1, 'Warehouse', plant)\n",
        "    out_df.insert(2, 'Transport_Mode', mode)\n",
        "\n",
        "    safe_mode = re.sub(r'\\W+', '_', mode).strip('_')\n",
        "    file_path = os.path.join(by_mode_dir,\n",
        "                             f'outbound_daily_totals_{plant}_{safe_mode}.csv')\n",
        "    out_df.to_csv(file_path, index=False)\n",
        "    print(f'✅  by_mode   → {file_path}')\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 4)  B)  WAREHOUSE-ALL (transport ignored)  →  2 files\n",
        "# ------------------------------------------------------------------\n",
        "for plant, g in df.groupby('PLANT_NAME'):\n",
        "    daily_tot = (\n",
        "        g.groupby(g['OUTBOUND_DATE'].dt.normalize())['NET_QUANTITY_MT']\n",
        "          .sum()\n",
        "          .rename('Total_NET_QUANTITY_MT')\n",
        "          .reindex(full_index, fill_value=0)\n",
        "    )\n",
        "\n",
        "    out_df = daily_tot.reset_index().rename(columns={'index': 'Date'})\n",
        "    out_df.insert(1, 'Warehouse', plant)\n",
        "\n",
        "    file_path = os.path.join(warehouse_all_dir,\n",
        "                             f'outbound_daily_totals_{plant}_ALL.csv')\n",
        "    out_df.to_csv(file_path, index=False)\n",
        "    print(f'✅  warehouse_all → {file_path}')\n",
        "\n",
        "print('\\n✔️  Done — transport-specific files in “by_mode/”, combined files in “warehouse_all/”.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGhhKDErjEvs",
        "outputId": "c6ecab1a-22ce-4ab0-cad6-d96af71be6ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅  by_mode   → /content/Outbound/by_mode/outbound_daily_totals_CHINA-WAREHOUSE_Truck.csv\n",
            "✅  by_mode   → /content/Outbound/by_mode/outbound_daily_totals_SINGAPORE-WAREHOUSE_Marine.csv\n",
            "✅  by_mode   → /content/Outbound/by_mode/outbound_daily_totals_SINGAPORE-WAREHOUSE_Truck.csv\n",
            "✅  warehouse_all → /content/Outbound/warehouse_all/outbound_daily_totals_CHINA-WAREHOUSE_ALL.csv\n",
            "✅  warehouse_all → /content/Outbound/warehouse_all/outbound_daily_totals_SINGAPORE-WAREHOUSE_ALL.csv\n",
            "\n",
            "✔️  Done — transport-specific files in “by_mode/”, combined files in “warehouse_all/”.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 1)  CONFIGURE PATHS\n",
        "# ------------------------------------------------------------------\n",
        "inbound_dir   = '/content/Inbound'                                          # daily inbound files\n",
        "outbound_dir  = '/content/Outbound/warehouse_all'                           # daily outbound (ALL)\n",
        "output_dir    = '/content/Inventory_Inbound_and_Outbound'                   # where to save the merged output\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2)  FUNCTION TO MERGE INBOUND AND OUTBOUND FOR A SPECIFIC WAREHOUSE\n",
        "# ------------------------------------------------------------------\n",
        "def merge_inbound_outbound(plant):\n",
        "    # --- daily inbound (e.g., inbound_daily_totals_SINGAPORE-WAREHOUSE.csv) ------------------------\n",
        "    in_file  = f'{inbound_dir}/inbound_daily_totals_{plant}-WAREHOUSE.csv'  # Corrected inbound file path\n",
        "    inbound  = pd.read_csv(in_file, parse_dates=['Date'])\n",
        "\n",
        "    # --- daily outbound (mode-agnostic) ----------------------------------------------\n",
        "    out_file = f'{outbound_dir}/outbound_daily_totals_{plant}-WAREHOUSE_ALL.csv'  # Corrected outbound file path\n",
        "    outbound = pd.read_csv(out_file, parse_dates=['Date'])\n",
        "\n",
        "    # --- merge both datasets on 'Date' ----------------------------------------------\n",
        "    merged = pd.merge(inbound, outbound, on='Date', how='outer', suffixes=('_Inbound', '_Outbound'))\n",
        "\n",
        "    # --- fill missing data with 0 and calculate leftover\n",
        "    merged['Inbound_MT'] = merged['Total_NET_QUANTITY_MT_Inbound'].fillna(0)\n",
        "    merged['Outbound_MT'] = merged['Total_NET_QUANTITY_MT_Outbound'].fillna(0)\n",
        "\n",
        "    # LeftOver = Inbound + (previous LeftOver) - Outbound\n",
        "    merged['LeftOver_MT'] = merged['Inbound_MT'] - merged['Outbound_MT']\n",
        "\n",
        "    # Add the 'Warehouse' column manually based on the `plant` name\n",
        "    merged['Warehouse'] = plant\n",
        "\n",
        "    # --- return merged DataFrame\n",
        "    return merged[['Date', 'Warehouse', 'Inbound_MT', 'Outbound_MT', 'LeftOver_MT']]\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 3)  PROCESS BOTH WAREHOUSES\n",
        "# ------------------------------------------------------------------\n",
        "for warehouse in ['SINGAPORE', 'CHINA']:\n",
        "    df_merged = merge_inbound_outbound(f'{warehouse}')  # Adjusted to match the correct naming pattern\n",
        "    out_path = os.path.join(output_dir, f'Inventory_Inbound_and_Outbound_{warehouse}.csv')\n",
        "    df_merged.to_csv(out_path, index=False)\n",
        "    print(f'✅  Saved {out_path}')\n",
        "\n",
        "print('\\n✔️  Files saved successfully in /content/Inventory_Inbound_and_Outbound folder.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKaziz92oSn9",
        "outputId": "cb5b2bef-2720-4446-948c-d22563d4fb86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅  Saved /content/Inventory_Inbound_and_Outbound/Inventory_Inbound_and_Outbound_SINGAPORE.csv\n",
            "✅  Saved /content/Inventory_Inbound_and_Outbound/Inventory_Inbound_and_Outbound_CHINA.csv\n",
            "\n",
            "✔️  Files saved successfully in /content/Inventory_Inbound_and_Outbound folder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "\n",
        "# Load the data\n",
        "url = 'https://docs.google.com/spreadsheets/d/1Y9UJwH5q6Xq6Xq6Xq6Xq6Xq6Xq6Xq6Xq6Xq6Xq6Xq6/export?format=xlsx'\n",
        "df = pd.read_excel('cOst.xlsx')\n",
        "\n",
        "# Convert date column to datetime\n",
        "df['DATE'] = pd.to_datetime(df['DATE'])\n",
        "\n",
        "# Create month column\n",
        "df['Month'] = df['DATE'].dt.to_period('M')\n",
        "\n",
        "# Calculate monthly sums\n",
        "monthly_sgn = df.groupby('Month')['SGN_COST'].sum().reset_index()\n",
        "monthly_cn = df.groupby('Month')['CN_COST'].sum().reset_index()\n",
        "monthly_sgn['Month'] = monthly_sgn['Month'].astype(str)\n",
        "monthly_cn['Month'] = monthly_cn['Month'].astype(str)\n",
        "\n",
        "# Calculate averages\n",
        "sgn_avg = df['SGN_COST'].mean()\n",
        "cn_avg = df['CN_COST'].mean()\n",
        "\n",
        "# Plot 1: Daily costs with averages and monthly bars\n",
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "# Plot monthly bars (background)\n",
        "months = df['DATE'].dt.to_period('M').unique()\n",
        "month_starts = [pd.Period(month, freq='M').start_time for month in months]\n",
        "month_ends = [pd.Period(month, freq='M').end_time for month in months]\n",
        "\n",
        "for start, end in zip(month_starts, month_ends):\n",
        "    monthly_sgn_sum = df[(df['DATE'] >= start) & (df['DATE'] <= end)]['SGN_COST'].sum()\n",
        "    monthly_cn_sum = df[(df['DATE'] >= start) & (df['DATE'] <= end)]['CN_COST'].sum()\n",
        "\n",
        "    plt.bar(start, monthly_sgn_sum, width=25, color='lightblue', alpha=0.3, label='SGN Monthly' if start == month_starts[0] else \"\")\n",
        "    plt.bar(start, monthly_cn_sum, width=25, color='lightgreen', alpha=0.3, label='CN Monthly' if start == month_starts[0] else \"\")\n",
        "\n",
        "# Plot daily lines\n",
        "plt.plot(df['DATE'], df['SGN_COST'], label='SGN Daily', color='blue')\n",
        "plt.plot(df['DATE'], df['CN_COST'], label='CN Daily', color='green')\n",
        "\n",
        "# Plot average lines\n",
        "plt.axhline(y=sgn_avg, color='blue', linestyle='--', label=f'SGN Avg: {sgn_avg:,.2f}')\n",
        "plt.axhline(y=cn_avg, color='green', linestyle='--', label=f'CN Avg: {cn_avg:,.2f}')\n",
        "\n",
        "# Formatting\n",
        "plt.title('Daily Costs with Monthly Totals and Averages')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Cost (THB)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Format y-axis to show full numbers\n",
        "def millions(x, pos):\n",
        "    return f'{x/1e6:.0f}M'\n",
        "plt.gca().yaxis.set_major_formatter(FuncFormatter(millions))\n",
        "\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot 2: Cumulative graph of both warehouses combined\n",
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "# Calculate combined cumulative\n",
        "df['Combined_Cumulative'] = df['SGN_CUMULATIVE'] + df['CN_CUMULATIVE']\n",
        "\n",
        "plt.plot(df['DATE'], df['Combined_Cumulative'], label='Combined Cumulative Cost', color='purple')\n",
        "\n",
        "# Formatting\n",
        "plt.title('Cumulative Cost of Both Warehouses')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Cumulative Cost (THB)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Format y-axis to show full numbers\n",
        "def billions(x, pos):\n",
        "    return f'{x/1e9:.1f}B'\n",
        "plt.gca().yaxis.set_major_formatter(FuncFormatter(billions))\n",
        "\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "aKkHMmXX3Eqk",
        "outputId": "9bddcd99-0fb3-4d4d-ba70-ed788d8ba355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'cOst.xlsx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7-277402005.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://docs.google.com/spreadsheets/d/1Y9UJwH5q6Xq6Xq6Xq6Xq6Xq6Xq6Xq6Xq6Xq6Xq6Xq6/export?format=xlsx'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cOst.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Convert date column to datetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m     ) as handle:\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cOst.xlsx'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load Inbound Data (already loaded earlier)\n",
        "inbound_df['INBOUND_DATE'] = pd.to_datetime(inbound_df['INBOUND_DATE'])\n",
        "\n",
        "# Filter data for relevant columns\n",
        "inbound_data = inbound_df[['INBOUND_DATE', 'PLANT_NAME', 'MATERIAL_NAME', 'NET_QUANTITY_MT']]\n",
        "\n",
        "# Feature Engineering: Extract month and year for seasonality\n",
        "inbound_data['Year'] = inbound_data['INBOUND_DATE'].dt.year\n",
        "inbound_data['Month'] = inbound_data['INBOUND_DATE'].dt.month\n",
        "inbound_data['Day'] = inbound_data['INBOUND_DATE'].dt.day\n",
        "\n",
        "# Creating lag features for previous month's inbound stock (Lag = 1)\n",
        "inbound_data['Lag_1'] = inbound_data.groupby('MATERIAL_NAME')['NET_QUANTITY_MT'].shift(1)\n",
        "\n",
        "# Drop rows with NaN values created by the lag feature\n",
        "inbound_data = inbound_data.dropna()\n",
        "\n",
        "# Filter data for training (Jan 2024 - Dec 2024)\n",
        "train_data = inbound_data[inbound_data['Year'] == 2024]\n",
        "\n",
        "# Define features and target variable\n",
        "X = train_data[['Month', 'Lag_1']]\n",
        "y = train_data['NET_QUANTITY_MT']\n",
        "\n",
        "# Train-test split for validation (We will train on 2024 data and forecast Jan 2025)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# XGBoost Model\n",
        "model = xgb.XGBRegressor(objective='reg:squarederror', colsample_bytree=0.3, learning_rate=0.1,\n",
        "                         max_depth=5, alpha=10, n_estimators=100)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f'Mean Absolute Error for Inbound Forecasting: {mae}')\n",
        "\n",
        "# Forecast Inbound for January 2025\n",
        "# Let's assume we are forecasting for one specific SKU (e.g., 'MAT-0045') and warehouse (e.g., 'SINGAPORE')\n",
        "# We will forecast for January 2025 with known month and previous month's inbound (Lag_1)\n",
        "\n",
        "forecast_data = {\n",
        "    'Month': [1],  # January 2025\n",
        "    'Lag_1': [inbound_data[inbound_data['MATERIAL_NAME'] == 'MAT-0045']['NET_QUANTITY_MT'].iloc[-1]]  # Use Dec 2024 inbound as Lag_1\n",
        "}\n",
        "\n",
        "forecast_df = pd.DataFrame(forecast_data)\n",
        "forecasted_inbound = model.predict(forecast_df)\n",
        "\n",
        "# Print the forecasted inbound quantity\n",
        "print(f\"Forecasted Inbound for Jan 2025 (MAT-0045): {forecasted_inbound[0]}\")\n",
        "\n",
        "# Plot the forecasted vs actual data (if actual data for Jan 2025 is available)\n",
        "# This will help visualize how well the forecast performed\n",
        "\n",
        "# Combine actual and forecasted data\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(inbound_data['INBOUND_DATE'], inbound_data['NET_QUANTITY_MT'], label='Actual Inbound', color='blue')\n",
        "plt.axvline(x=pd.to_datetime('2025-01-01'), color='red', linestyle='--', label='Forecast Start (Jan 2025)')\n",
        "\n",
        "# Forecasting for January 2025\n",
        "plt.scatter(pd.to_datetime('2025-01-01'), forecasted_inbound[0], color='green', label=f'Forecasted Jan 2025 (Inbound: {forecasted_inbound[0]:.2f} MT)')\n",
        "\n",
        "plt.title('Inbound Forecast vs Actual (Jan 2025)')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Inbound Quantity (MT)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Sg1iQiimBZhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "18fSOLO3qE0i"
      }
    }
  ]
}